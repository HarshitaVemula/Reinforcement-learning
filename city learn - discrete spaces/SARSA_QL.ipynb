{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn import  CityLearn, building_loader, auto_size\n",
    "from energy_models import HeatPump, EnergyStorage, Building\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "np.random.seed(3)\n",
    "\n",
    "import ray \n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only one building for SINGLE AGENT environment, unmark multiple building IDs to simulate MULTI-AGENT environment. In the multi-agent environment\n",
    "#the reward of each agent depend partially on the actions of the other agents or buildings (see reward_function.py)\n",
    "building_ids = [8]#, 5, 9, 16, 21, 26, 33, 36, 49, 59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.00000000e-01 -2.00000000e-01 -1.00000000e-01  5.55111512e-17\n",
      "  1.00000000e-01  2.00000000e-01  3.00000000e-01  4.00000000e-01]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Building the RL environment with heating and cooling loads and weather files\n",
    "CityLearn\n",
    "    Weather file\n",
    "    Buildings\n",
    "        File with heating and cooling demands\n",
    "        CoolingDevices (HeatPump)\n",
    "        CoolingStorages (EnergyStorage)\n",
    "'''\n",
    "\n",
    "data_folder = Path(\"data/\")\n",
    "\n",
    "demand_file = data_folder / \"AustinResidential_TH.csv\"\n",
    "weather_file = data_folder / 'Austin_Airp_TX-hour.csv'\n",
    "\n",
    "heat_pump, heat_tank, cooling_tank = {}, {}, {}\n",
    "\n",
    "#Ref: Assessment of energy efficiency in electric storage water heaters (2008 Energy and Buildings)\n",
    "loss_factor = 0.19/24\n",
    "buildings = []\n",
    "for uid in building_ids:\n",
    "    heat_pump[uid] = HeatPump(nominal_power = 9e12, eta_tech = 0.22, t_target_heating = 45, t_target_cooling = 10)\n",
    "    heat_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    cooling_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    buildings.append(Building(uid, heating_storage = heat_tank[uid], cooling_storage = cooling_tank[uid], heating_device = heat_pump[uid], cooling_device = heat_pump[uid]))\n",
    "    buildings[-1].state_space(np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1, 40.0, 1.001]), np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 17.0, -0.001]))\n",
    "    buildings[-1].action_space(np.array([0.5]), np.array([-0.3]))\n",
    "    \n",
    "building_loader(demand_file, weather_file, buildings)  \n",
    "auto_size(buildings, t_target_heating = 45, t_target_cooling = 10)\n",
    "\n",
    "env = CityLearn(demand_file, weather_file, buildings = buildings, time_resolution = 1, simulation_period = (3500,6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_function import reward_function\n",
    "observations_space, actions_space = [],[]\n",
    "for building in buildings:\n",
    "    observations_space.append(building.observation_spaces)\n",
    "    actions_space.append(building.action_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_function import reward_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.9\n",
    "w=np.ones(24+8)\n",
    "\n",
    "def generate_state_action_matrix(state,action_shape):\n",
    "    ns=len(state)\n",
    "    x_s_a=np.zeros((action_shape,ns+action_shape))\n",
    "    for i in range(action_shape):\n",
    "        l=np.zeros(action_shape)\n",
    "        l[i]=1\n",
    "        x_s_a[i,:]=np.concatenate([np.array(state),l])\n",
    "    return(x_s_a)\n",
    "    \n",
    "def get_q_values(x_s_a,w):\n",
    "    na=x_s_a.shape[0]\n",
    "    ns_na=x_s_a.shape[1]\n",
    "    q_s_a=np.zeros(na)\n",
    "    for i in range(na):\n",
    "        #print(w.reshape(1,32).shape)\n",
    "        #print(x_s_a[0,:].reshape(32,1).shape)\n",
    "        q_s_a[i]=np.matmul(w.reshape(1,ns_na),x_s_a[i,:].reshape(ns_na,1))\n",
    "        #print(q_s_a[i])\n",
    "    return(q_s_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.00000000e-01, -2.00000000e-01, -1.00000000e-01,  5.55111512e-17,\n",
       "        1.00000000e-01,  2.00000000e-01,  3.00000000e-01,  4.00000000e-01])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space=np.arange(-0.3,0.5,0.1)\n",
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209.45633242157078\n",
      "207.79708014301963\n",
      "212.91057209964603\n",
      "215.87820965597064\n",
      "208.76994143248322\n",
      "209.58828467767987\n",
      "209.81606503279212\n",
      "209.1824008933386\n",
      "212.85672561944767\n",
      "211.93615806702576\n",
      "206.58280187793093\n",
      "214.453297283236\n",
      "209.3439791175629\n",
      "213.2629431353172\n",
      "210.7669777213256\n",
      "214.19528807366063\n",
      "209.66856026299118\n",
      "211.043631403325\n",
      "211.95450445279639\n",
      "207.68568509317348\n",
      "207.6896114752634\n",
      "211.71039069141568\n",
      "208.7884613345101\n",
      "213.40713102300717\n",
      "210.68526247574806\n",
      "213.47169311329355\n",
      "211.0302076925484\n",
      "214.07172795661333\n",
      "211.3828654175321\n",
      "211.8865258929652\n",
      "209.9543186365555\n",
      "210.6980112989772\n",
      "212.07330482066237\n",
      "213.3428237207736\n",
      "206.5225129033473\n",
      "210.50196558405713\n",
      "212.9327015333381\n",
      "210.55848228730963\n",
      "209.15268610190793\n",
      "212.6062453858052\n",
      "212.40819617232222\n",
      "210.38517969933645\n",
      "211.09370565000813\n",
      "213.50970185771033\n",
      "209.90619850002528\n",
      "212.0414742040319\n",
      "212.90959785587717\n",
      "214.64663938882256\n",
      "212.68493893342037\n",
      "213.75136350809328\n",
      "209.5415986646015\n",
      "212.0320942128574\n",
      "210.84236534610437\n",
      "209.88723223765697\n",
      "215.84077833806376\n",
      "207.49092587285654\n",
      "209.2061388371663\n",
      "209.35536159221297\n",
      "213.87948187819612\n",
      "212.39600883937524\n",
      "210.2783639123882\n",
      "209.34026419734394\n",
      "209.20108478558652\n",
      "209.89434111855778\n",
      "208.86942454202685\n",
      "207.1688352569144\n",
      "208.38190036398106\n",
      "206.47869105557118\n",
      "213.5793738232556\n",
      "214.3167127613171\n",
      "211.84189371570451\n",
      "210.56994493832488\n",
      "207.56853201001948\n",
      "210.1296310312776\n",
      "213.3346757062903\n",
      "213.59094908385435\n",
      "215.04708872566667\n",
      "211.07327259278168\n",
      "209.65121682688775\n",
      "211.187415466156\n",
      "212.4721275252332\n",
      "211.86107222352408\n",
      "212.22058764133806\n",
      "210.1616832206642\n",
      "214.83348770331406\n",
      "213.71345639093545\n",
      "211.68813400247046\n",
      "211.06527762369134\n",
      "214.6903718247315\n",
      "211.98540826690203\n",
      "211.66229796414368\n",
      "212.27217242185924\n",
      "209.244595997508\n",
      "213.51454340019214\n",
      "209.2322186313418\n",
      "211.12359700516254\n",
      "212.2462373497054\n",
      "211.67000867962398\n",
      "207.65785210924193\n",
      "214.00555290940235\n"
     ]
    }
   ],
   "source": [
    "#on policy sarsa\n",
    "#on policy sarsa with epsilon greedy much worse than greedy\n",
    "cost, cum_reward = {}, {}\n",
    "gamma=0.9\n",
    "alpha=0.1\n",
    "na=8\n",
    "for ep in range(100):\n",
    "    q=[]\n",
    "    states=[]\n",
    "    state = env.reset()\n",
    "    state=state[0][:24]\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    x_s_a=generate_state_action_matrix(state,8)\n",
    "    q_s_a_s=get_q_values(x_s_a,w)\n",
    "    action=action_space[np.argmax(q_s_a_s)]\n",
    "    x=x_s_a[np.argmax(q_s_a_s)]\n",
    "    q.append(np.max(q_s_a_s))\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step([[action]])\n",
    "        reward = reward_function(reward)[0] \n",
    "        #print('rea',reward)\n",
    "        state = next_state[0][:24]\n",
    "        \n",
    "        x_s_dash_a=generate_state_action_matrix(state,8)\n",
    "        q_s_dash_a_s=get_q_values(x_s_dash_a,w)\n",
    "        \n",
    "        epsilon=np.random.rand(1)\n",
    "        if epsilon<0.7:\n",
    "            action_dash=action_space[np.argmax(q_s_dash_a_s)]\n",
    "            q_s_dash=np.max(q_s_dash_a_s)\n",
    "            x_=x_s_dash_a[np.argmax(q_s_dash_a_s)]\n",
    "            \n",
    "        else:\n",
    "            ac=np.random.choice(na)\n",
    "            action_dash=action_space[ac]\n",
    "            q_s_dash=q_s_dash_a_s[ac]\n",
    "            x_=x_s_dash_a[ac]\n",
    "        #print(np.matmul(w,x))\n",
    "        #print(np.matmul(w,x_s_dash_a[np.argmax(q_s_dash_a_s)]))\n",
    "        q_s=q[-1]\n",
    "        w=w+alpha*(reward+gamma*q_s_dash-q_s)*(gamma*x_-x)\n",
    "        \n",
    "        q.append(q_s_dash)\n",
    "        \n",
    "        action=action_dash\n",
    "        x=x_\n",
    "    cost[ep] = env.cost()\n",
    "    print(cost[ep])\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.40900847061246\n",
      "180.53393503856896\n",
      "180.66307259918216\n",
      "180.52080049387337\n",
      "180.49528783022978\n",
      "180.47274440159893\n",
      "180.434599219371\n",
      "180.5257305804471\n",
      "180.52994307696972\n",
      "180.62521659615845\n",
      "180.52080049387337\n",
      "180.49528783022978\n",
      "180.47274440159893\n",
      "180.434599219371\n",
      "180.5257305804471\n",
      "180.52994307696972\n",
      "180.62521659615845\n",
      "180.52080049387337\n",
      "180.49528783022978\n",
      "180.47274440159893\n",
      "180.434599219371\n",
      "180.5257305804471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in double_scalars\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.3170853281487\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n",
      "180.25737295269303\n"
     ]
    }
   ],
   "source": [
    "#sarsa greedy\n",
    "cost, cum_reward = {}, {}\n",
    "gamma=0.9\n",
    "alpha=0.1\n",
    "na=8\n",
    "w=np.ones(32)\n",
    "for ep in range(100):\n",
    "    q=[]\n",
    "    states=[]\n",
    "    state = env.reset()\n",
    "    state=state[0][:24]\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    x_s_a=generate_state_action_matrix(state,8)\n",
    "    q_s_a_s=get_q_values(x_s_a,w)\n",
    "    action=action_space[np.argmax(q_s_a_s)]\n",
    "    x=x_s_a[np.argmax(q_s_a_s)]\n",
    "    q.append(np.max(q_s_a_s))\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step([[action]])\n",
    "        reward = reward_function(reward)[0] \n",
    "        #print('rea',reward)\n",
    "        state = next_state[0][:24]\n",
    "        \n",
    "        x_s_dash_a=generate_state_action_matrix(state,8)\n",
    "        q_s_dash_a_s=get_q_values(x_s_dash_a,w)\n",
    "        \n",
    "        epsilon=np.random.rand(1)\n",
    "        #if epsilon<0.7:\n",
    "        action_dash=action_space[np.argmax(q_s_dash_a_s)]\n",
    "        q_s_dash=np.max(q_s_dash_a_s)\n",
    "        x_=x_s_dash_a[np.argmax(q_s_dash_a_s)]\n",
    "            \n",
    "#         else:\n",
    "#             ac=np.random.choice(na)\n",
    "#             action_dash=action_space[ac]\n",
    "#             q_s_dash=q_s_dash_a_s[ac]\n",
    "#             x_=x_s_dash_a[ac]\n",
    "        #print(np.matmul(w,x))\n",
    "        #print(np.matmul(w,x_s_dash_a[np.argmax(q_s_dash_a_s)]))\n",
    "        q_s=q[-1]\n",
    "        w=w+alpha*(reward+gamma*q_s_dash-q_s)*(gamma*x_-x)\n",
    "        \n",
    "        q.append(q_s_dash)\n",
    "        \n",
    "        action=action_dash\n",
    "        x=x_\n",
    "    cost[ep] = env.cost()\n",
    "    print(cost[ep])\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.7785537611777\n",
      "195.87893517418928\n",
      "197.31352797014839\n",
      "196.25813051958207\n",
      "194.84779475397622\n",
      "195.47935961556\n",
      "195.41635263119434\n",
      "195.02502920559255\n",
      "195.05472343671522\n",
      "196.06723487103153\n",
      "196.60921795038834\n",
      "197.0381282875192\n",
      "195.70548139389857\n",
      "194.91254911564752\n",
      "195.37059015725123\n",
      "195.2383221258765\n",
      "195.50643750011403\n",
      "196.08142165924005\n",
      "194.36577593074261\n",
      "195.702408206766\n",
      "196.1813634084208\n",
      "195.10260042612197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: overflow encountered in double_scalars\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204.4504002066826\n",
      "212.21939707456477\n",
      "206.05481775015213\n",
      "210.26302373255905\n",
      "210.8358306629208\n",
      "210.1672014531316\n",
      "210.27405438587272\n",
      "214.1022604052116\n",
      "214.82426070308873\n",
      "211.6755645439997\n",
      "212.8408300352563\n",
      "211.4525275681009\n",
      "214.7246346057107\n",
      "212.6084896541909\n",
      "214.53243791277663\n",
      "210.5907491950985\n",
      "213.13451297009874\n",
      "211.61213750362012\n",
      "212.26357596786434\n",
      "212.31616898825652\n",
      "211.03088113231914\n",
      "210.27595003849373\n",
      "211.080874502206\n",
      "210.3424342078972\n",
      "210.4376143255399\n",
      "213.20713861923196\n",
      "210.32883585368074\n",
      "211.19812658735157\n",
      "213.60847667326493\n",
      "214.83895312158296\n",
      "209.79274954147462\n",
      "212.18296447750265\n",
      "212.2783337516782\n",
      "214.83981473012022\n",
      "209.6105243787246\n",
      "209.77321862969552\n",
      "208.0574583606622\n",
      "216.65203808574105\n",
      "213.57866693705733\n",
      "212.55831316553153\n",
      "211.96614624194987\n",
      "206.57469309570416\n",
      "211.62979072025746\n",
      "209.61803635310238\n",
      "209.3438867415649\n",
      "213.35301851933067\n",
      "211.4229131778351\n",
      "209.05881392804207\n",
      "212.39510880384825\n",
      "208.1255381894592\n",
      "210.30904634521949\n",
      "212.6404526782735\n",
      "213.31702230508157\n",
      "213.45946171894104\n",
      "211.1174247423507\n",
      "212.251345340237\n",
      "211.21090251805458\n",
      "213.62280375033876\n",
      "215.1272500361972\n",
      "209.36373001072093\n",
      "211.7638438558465\n",
      "209.66941831033475\n",
      "213.03875839628657\n",
      "212.25480668472434\n",
      "213.00427413615384\n",
      "210.21675865245035\n",
      "211.28122963951415\n",
      "214.48335374588984\n",
      "210.07614537052507\n",
      "213.0024858614682\n",
      "211.2298864675363\n",
      "211.5202789360637\n",
      "211.52338263459464\n",
      "209.98180111976959\n",
      "213.44405524965305\n",
      "212.37198738548312\n",
      "211.73374297250456\n",
      "207.57724625300617\n"
     ]
    }
   ],
   "source": [
    "#Qlearning\n",
    "cost, cum_reward = {}, {}\n",
    "gamma=0.9\n",
    "alpha=0.1\n",
    "na=8\n",
    "for ep in range(100):\n",
    "    q=[]\n",
    "    states=[]\n",
    "    state = env.reset()\n",
    "    state=state[0][:24]\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    x_s_a=generate_state_action_matrix(state,8)\n",
    "    q_s_a_s=get_q_values(x_s_a,w)\n",
    "    action=action_space[np.argmax(q_s_a_s)]\n",
    "    x=x_s_a[np.argmax(q_s_a_s)]\n",
    "    q.append(np.max(q_s_a_s))\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step([[action]])\n",
    "        reward = reward_function(reward)[0] \n",
    "        state = next_state[0][:24]\n",
    "        \n",
    "        x_s_dash_a=generate_state_action_matrix(state,8)\n",
    "        q_s_dash_a_s=get_q_values(x_s_dash_a,w)\n",
    "        \n",
    "        epsilon=np.random.rand(1)\n",
    "        if epsilon<0.7:\n",
    "            action_dash=action_space[np.argmax(q_s_dash_a_s)]\n",
    "            \n",
    "        else:\n",
    "            ac=np.random.choice(na)\n",
    "            action_dash=action_space[ac]\n",
    "        p=np.argmax(q_s_dash_a_s)    \n",
    "        q_s_dash=np.max(q_s_dash_a_s)\n",
    "        x_=x_s_dash_a[p]\n",
    "        #print(np.matmul(w,x))\n",
    "        #print(np.matmul(w,x_s_dash_a[np.argmax(q_s_dash_a_s)]))\n",
    "        q_s=q[-1]\n",
    "        w=w+alpha*(reward+gamma*q_s_dash-q_s)*(gamma*x_-x)\n",
    "        \n",
    "        q.append(q_s_dash)\n",
    "        \n",
    "        action=action_dash\n",
    "        x=x_\n",
    "    cost[ep] = env.cost()\n",
    "    print(cost[ep])\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(np.exp(x)/(1+np.exp(x)))\n",
    "\n",
    "\n",
    "def derivative_log_sigmoid(x):\n",
    "    der=(1/(sigmoid(x)-0.5))*sigmoid(x)*(1-sigmoid(x))\n",
    "    return(x)\n",
    "\n",
    "def montecarlo_return(rewards,gamma):\n",
    "    l=len(rewards)\n",
    "    r=0\n",
    "    for i in reversed(range(l)):\n",
    "        r=rewards[i]+(gamma**(l-i-1))*r\n",
    "    return(r)\n",
    "\n",
    "\n",
    "\n",
    "theta=np.random.rand(24)\n",
    "w=np.random.rand(24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.753987398135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.8592577492545\n",
      "188.7816348913295\n",
      "244.317406385705\n",
      "234.42513284241028\n",
      "242.2942962262748\n",
      "202.4731106976577\n",
      "255.44317297750828\n",
      "211.88503578061324\n",
      "246.54452303867114\n"
     ]
    }
   ],
   "source": [
    "#reinforce montecarlo with baseline\n",
    "#problem:The agent is not able to pick up the action\n",
    "#action=tanh(xB)\n",
    "from reward_function import reward_function\n",
    "k = 0\n",
    "cost, cum_reward = {}, {}\n",
    "max_action=0.2\n",
    "episodes = 10\n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "\n",
    "for ep in range(10):\n",
    "    rewards_=[]\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    \n",
    "    state = env.reset()\n",
    "    state=state[0][:24]\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    t=0\n",
    "    #print(states)\n",
    "    j=0\n",
    "    while not done:\n",
    "        #if k%500==0:\n",
    "            #print('hour: '+str(k)+' of '+str(2500*episodes))\n",
    "        x=np.matmul(theta.T,state.T)\n",
    "        action = sigmoid(x)-0.5\n",
    "        next_state, reward, done, _ = env.step([[action]])\n",
    "        reward = reward_function(reward)[0] \n",
    "        \n",
    "        state = next_state[0][:24]\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_.append(reward)\n",
    "        \n",
    "        \n",
    "        t=t+1\n",
    "        if done==True:\n",
    "            T=t\n",
    "        k+=1\n",
    "        \n",
    "    for t in range(T):\n",
    "        G=montecarlo_return(rewards_[t:],1)\n",
    "        s=states[t]\n",
    "        x=np.matmul(theta.reshape(1,24),s.reshape(24,1))[0][0]\n",
    "        v_s=np.matmul(w.reshape(1,24),s.reshape(24,1))[0][0]\n",
    "        delta=G-v_s\n",
    "        \n",
    "        w=w+(alpha*delta*s)\n",
    "        \n",
    "        der=derivative_log_sigmoid(x)\n",
    "        theta=theta+alpha*(gamma**t)*delta*der*s\n",
    "    cost[ep] = env.cost()\n",
    "    #print(theta)\n",
    "    print(cost[ep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one step actor critic\n",
    "def sigmoid(x):\n",
    "    return(np.exp(x)/(1+np.exp(x)))\n",
    "\n",
    "\n",
    "def derivative_log_sigmoid(x):\n",
    "    der=(1/(sigmoid(x)-0.5))*sigmoid(x)*(1-sigmoid(x))\n",
    "    return(x)\n",
    "\n",
    "def montecarlo_return(rewards,gamma):\n",
    "    l=len(rewards)\n",
    "    r=0\n",
    "    for i in reversed(range(l)):\n",
    "        r=rewards[i]+(gamma**(l-i-1))*r\n",
    "    return(r)\n",
    "\n",
    "\n",
    "theta=np.random.rand(24)\n",
    "w=np.random.rand(24)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.87086419230135\n",
      "186.86431891754765\n",
      "186.8803099509173\n",
      "186.8994409971376\n",
      "186.92224113529005\n",
      "186.95467533034733\n",
      "186.98178422358177\n",
      "187.0409623059019\n",
      "187.1538654358502\n",
      "187.2544520786789\n"
     ]
    }
   ],
   "source": [
    "from reward_function import reward_function\n",
    "k = 0\n",
    "cost, cum_reward = {}, {}\n",
    "max_action=0.2\n",
    "episodes = 10\n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "\n",
    "\n",
    "for ep in range(1000):\n",
    "    rewards_=[]\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    \n",
    "    state = env.reset()\n",
    "    state=state[0][:24]\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    I=1\n",
    "    t=0\n",
    "    while not done:\n",
    "        #if k%500==0:\n",
    "            #print('hour: '+str(k)+' of '+str(2500*episodes))\n",
    "        s=state\n",
    "        x=np.matmul(theta.T,state.T)\n",
    "        action = sigmoid(x)-0.5\n",
    "        #print(action)\n",
    "        next_state, reward, done, _ = env.step([[action]])\n",
    "        reward = reward_function(reward)[0] \n",
    "        #print(reward)\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        rewards_.append(reward)\n",
    "        \n",
    "        next_s=next_state[0][:24]\n",
    "        v_nextstate=np.matmul(w.reshape(1,24),next_s.reshape(24,1))[0][0]\n",
    "        v_state=np.matmul(w.reshape(1,24),s.reshape(24,1))[0][0]\n",
    "        \n",
    "        delta=reward+gamma*v_nextstate-v_state\n",
    "        #print(delta)\n",
    "        w=w+alpha*delta*s\n",
    "        \n",
    "        der=derivative_log_sigmoid(x)\n",
    "        theta=theta+alpha*I*delta*der*s\n",
    "        I=gamma*I\n",
    "        state = next_s\n",
    "        \n",
    "        t=t+1\n",
    "        if done==True:\n",
    "            T=t\n",
    "        k+=1\n",
    "   \n",
    "    cost[ep] = env.cost()\n",
    "    #print(theta)\n",
    "    if ep%100==0:\n",
    "        print(cost[ep])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one step actor critic\n",
    "def sigmoid(x):\n",
    "    return(np.exp(x)/(1+np.exp(x)))\n",
    "\n",
    "\n",
    "def derivative_log_sigmoid(x):\n",
    "    der=(1/(sigmoid(x)-0.5))*sigmoid(x)*(1-sigmoid(x))\n",
    "    return(x)\n",
    "\n",
    "def montecarlo_return(rewards,gamma):\n",
    "    l=len(rewards)\n",
    "    r=0\n",
    "    for i in reversed(range(l)):\n",
    "        r=rewards[i]+(gamma**(l-i-1))*r\n",
    "    return(r)\n",
    "\n",
    "\n",
    "theta=np.random.rand(24)\n",
    "w=np.random.rand(24)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.72650702715777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b55ec485af31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/harshita/Academics/semester 3/citylearnmajor/project/citylearn.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_resolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;31m#Heating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mbuilding_electric_demand\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbuilding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_storage_heating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0;31m#Cooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mbuilding_electric_demand\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbuilding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_storage_cooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/harshita/Academics/semester 3/citylearnmajor/project/energy_models.py\u001b[0m in \u001b[0;36mset_storage_heating\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mheat_power_avail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheating_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_heating_power\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_source_heating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't_out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'heating_demand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mheating_energy_balance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheating_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'heating_demand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheat_power_avail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheating_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapacity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mheating_energy_balance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheating_energy_balance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'heating_demand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0melec_demand_heating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheating_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_electric_consumption_heating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheat_supply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheating_energy_balance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melectricity_consumption_heating\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melec_demand_heating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4370\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4372\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'getitem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         return (super(Int64Index, self)\n\u001b[0;32m--> 211\u001b[0;31m                 ._convert_scalar_indexer(key, kind=kind))\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_joined_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m   2849\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_index_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_convert_scalar_indexer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'getitem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#actor critic with eligibility traces\n",
    "from reward_function import reward_function\n",
    "k = 0\n",
    "cost, cum_reward = {}, {}\n",
    "max_action=0.2\n",
    "episodes = 10\n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "\n",
    "\n",
    "for ep in range(1000):\n",
    "    rewards_=[]\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    \n",
    "    state = env.reset()\n",
    "    state=state[0][:24]\n",
    "    states.append(state)\n",
    "    done = False\n",
    "\n",
    "    t=0\n",
    "    z_w=np.zeros(24)\n",
    "    z_theta=np.zeros(24)\n",
    "    R_bar=0\n",
    "    lambda_w=0.8\n",
    "    lambda_t=0.8\n",
    "    while not done:\n",
    "        #if k%500==0:\n",
    "            #print('hour: '+str(k)+' of '+str(2500*episodes))\n",
    "        s=state\n",
    "        x=np.matmul(theta.T,state.T)\n",
    "        action = sigmoid(x)-0.5\n",
    "        #print(action)\n",
    "        next_state, reward, done, _ = env.step([[action]])\n",
    "        reward = reward_function(reward)[0] \n",
    "        #print(reward)\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        rewards_.append(reward)\n",
    "        \n",
    "        next_s=next_state[0][:24]\n",
    "        v_nextstate=np.matmul(w.reshape(1,24),next_s.reshape(24,1))[0][0]\n",
    "        v_state=np.matmul(w.reshape(1,24),s.reshape(24,1))[0][0]\n",
    "        \n",
    "        delta=reward+gamma*v_nextstate-v_state-R_bar\n",
    "        der=derivative_log_sigmoid(x)\n",
    "        \n",
    "        R_bar=R_bar+alpha*delta\n",
    "        z_w=z_w*lambda_w+s\n",
    "        z_theta=z_theta*lambda_t+der*s\n",
    "        #print(delta)\n",
    "        w=w+alpha*delta*z_w\n",
    "        theta=theta+alpha*I*delta*der*z_theta\n",
    "\n",
    "        state = next_s\n",
    "        \n",
    "        t=t+1\n",
    "        if done==True:\n",
    "            T=t\n",
    "        k+=1\n",
    "   \n",
    "    cost[ep] = env.cost()\n",
    "    #print(theta)\n",
    "    if ep%100==0:\n",
    "        print(cost[ep])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discrete action space-q learning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
