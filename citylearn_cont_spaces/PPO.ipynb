{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn import  CityLearn, building_loader, auto_size\n",
    "from energy_models import HeatPump, EnergyStorage, Building\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATE VARIABLES\n",
    "s1: outdoor temperature in Celcius degrees. This state is the same for every building for each time step.\n",
    "s2: hour of day (from 1 to 24). This state is the same for every building for each time step.\n",
    "s3: state of the charge of the energy storage device. From 0 (empty of cooling energy) to 1 (full of cooling energy). This state is the different for every building and depends on the actions taken by every agent (one agent per building, unless a centralized RL agent is designed to control all the buildings).\n",
    "\n",
    "ACTION VARIABLE\n",
    "a: increase (+) or decrease (-) of the amount of cooling energy stored in the energy storage device. From -0.5 (try to decrease the cooling energy stored in the storage device by an amount equivalent to 0.5 times its maximum capacity) to 0.5 (try to increase the cooling energy stored in the storage device by an amount equivalent to 0.5 times its maximum capacity). In order to decrease the energy stored in the device, the energy must be released into the building. Therefore, s3 may not decrease by the same amount than the action taken if the demand for cooling energy of the building is lower than the amount of energy the RL agent is trying to release from the storage device.\n",
    "\n",
    "REWARD\n",
    "r: -cost of electricity. See reward_function.py, which contains a function that wraps the rewards obtained from the environment. The reward function can be modified by the user in order to minimize the cost function of the environment. There may exist alternative functions different than the cost function sqrt(sum(e^2)) that have the same minima than sqrt(sum(e^2)) (i.e. penalizing the RL agent with the maximum value of electricity consumption of each day may lead to the flattening of the curve of electricity consumption too).\n",
    "\n",
    "COST FUNCTION\n",
    "env.cost(): sqrt(sum(e^2)). Where 'e' is the sum of the  electricity consumption of all the buildings in a given time-step, and sum(e^2) is the sum of the squares of 'e' over the whole simulation period. The objetive of the agent(s) must be to minimize this cost.\n",
    "\n",
    "Any amount of cooling demand of the building that isn't satisfied by the energy storage device is automatically supplied by the heat pump directly to the building. The heat pump is the device that consumes electricity from the grid (and that has a direct impact on the rewards). The heat pump is more efficient (has a higher COP) if the outdoor air temperature is lower (s2), and less efficient (lower COP) when the outdoor temperature is higher (typically during the day time). On the other hand, the demand for cooling in the building is higher during the daytime and lower at night. COP = cooling_energy_generated/electricity_consumed. COP > 1.\n",
    "\n",
    "SINGLE-BUILDING\n",
    "For a single building, the optimal policy consists on storing cooling energy during the night (when the cooling demand of the building is low and the COP of the heat pump is higher), and releasing the stored cooling energy into the building during the day (high demand for cooling and low COP).\n",
    "MULTIPLE-BUILDINGS\n",
    "If multiple buildings are controlled independently of each other and with no coordination, they will all tend to consume more electricity simultaneously during the same hours at night (when the COPs are highest), raising the price for electricity that they all pay at this time and therefore the electricity cost won't be completely minimized.\n",
    "\n",
    "SINGLE AND MULTI-AGENT RL: Proposed challenges\n",
    "1- Implement an independent RL agent for every building (this has already been done in this example) and try to minimize the scores in the minimum number of episodes for multiple buildings running simultaneously. The algorithm should be properly calibrated to maximize its likelyhood of converging to a good policy (the current example does not converge 100% of the times it is run).\n",
    "2- Coordinate multiple decentralized RL agents or a single centralized agent to control all the buildings. The agents could share certain information with each other (i.e. s3), while other variables (i.e. s1 and s2) are aleady common for all the agents. The agents could decide which actions to take sequentially and share this information whith other agents so they can decide what actions they will take. Pay especial attention to whether the environment (as seen by every agent) follows the Markov property or not, and how the states should be defined accordingly such that it is as Markovian as possible.\n",
    "\n",
    "Unmark only one building ID for SINGLE AGENT environment, unmark multiple building IDs to simulate MULTI-AGENT environment.\n",
    "This main file originally contains the implementation of a DDPG RL agent to control a single building/agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only one building for SINGLE AGENT environment, unmark multiple building IDs to simulate MULTI-AGENT environment. In the multi-agent environment\n",
    "#the reward of each agent depend partially on the actions of the other agents or buildings (see reward_function.py)\n",
    "building_ids = [8]#, 5, 9, 16, 21, 26, 33, 36, 49, 59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building the RL environment with heating and cooling loads and weather files\n",
    "CityLearn\n",
    "    Weather file\n",
    "    Buildings\n",
    "        File with heating and cooling demands\n",
    "        CoolingDevices (HeatPump)\n",
    "        CoolingStorages (EnergyStorage)\n",
    "'''\n",
    "\n",
    "data_folder = Path(\"data/\")\n",
    "\n",
    "demand_file = data_folder / \"AustinResidential_TH.csv\"\n",
    "weather_file = data_folder / 'Austin_Airp_TX-hour.csv'\n",
    "\n",
    "heat_pump, heat_tank, cooling_tank = {}, {}, {}\n",
    "\n",
    "#Ref: Assessment of energy efficiency in electric storage water heaters (2008 Energy and Buildings)\n",
    "loss_factor = 0.19/24\n",
    "buildings = []\n",
    "for uid in building_ids:\n",
    "    heat_pump[uid] = HeatPump(nominal_power = 9e12, eta_tech = 0.22, t_target_heating = 45, t_target_cooling = 10)\n",
    "    heat_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    cooling_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    buildings.append(Building(uid, heating_storage = heat_tank[uid], cooling_storage = cooling_tank[uid], heating_device = heat_pump[uid], cooling_device = heat_pump[uid]))\n",
    "    buildings[-1].state_space(np.array([24.0, 40.0, 1.001]), np.array([1.0, 17.0, -0.001]))\n",
    "    buildings[-1].action_space(np.array([0.2]), np.array([-0.2]))\n",
    "    \n",
    "building_loader(demand_file, weather_file, buildings)  \n",
    "auto_size(buildings, t_target_heating = 45, t_target_cooling = 10)\n",
    "\n",
    "env = CityLearn(demand_file, weather_file, buildings = buildings, time_resolution = 1, simulation_period = (3500,6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "            nn.Hardtanh(min_val=-0.1,max_val=0.1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(1/(1+np.exp(torch.ones(1, num_outputs) * std*(-1))))\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        #print(std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1]  - values[step]\n",
    "        gae = delta + gamma * tau  * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = observations_space[-1].shape[0]\n",
    "num_outputs = actions_space[-1].shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 30000\n",
    "frame_idx  = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0612]])\n",
      "256.5764031429233\n",
      "tensor([[0.6599]])\n",
      "259.81406474239634\n",
      "tensor([[-0.3525]])\n",
      "257.5396248113071\n",
      "tensor([[-0.1323]])\n",
      "256.7020659523632\n",
      "tensor([[-0.6344]])\n",
      "254.9827481326525\n",
      "tensor([[-0.4351]])\n",
      "257.27596723732046\n",
      "tensor([[0.6430]])\n",
      "262.94917401117596\n",
      "tensor([[0.2355]])\n",
      "251.45418877626466\n",
      "tensor([[-0.6132]])\n",
      "256.5289945941509\n",
      "tensor([[0.0927]])\n",
      "258.08202345859377\n",
      "tensor([[-0.6442]])\n",
      "253.79125173308495\n",
      "tensor([[-0.4190]])\n",
      "255.0303243382151\n",
      "tensor([[0.0289]])\n",
      "252.13841666591392\n",
      "tensor([[0.3335]])\n",
      "253.38124444176512\n",
      "tensor([[-0.4018]])\n",
      "250.3892599113234\n",
      "tensor([[-0.3371]])\n",
      "252.67259324859\n",
      "tensor([[0.1866]])\n",
      "252.4369373428275\n",
      "tensor([[-0.1798]])\n",
      "255.63142875772317\n",
      "tensor([[0.1810]])\n",
      "253.41485872963588\n",
      "tensor([[-0.3965]])\n",
      "258.39095144913017\n",
      "tensor([[0.0841]])\n",
      "250.88338796149233\n",
      "tensor([[0.5453]])\n",
      "253.7371610782774\n",
      "tensor([[0.0754]])\n",
      "254.23019374845907\n",
      "tensor([[-1.0618]])\n",
      "255.289022430643\n",
      "tensor([[-0.2810]])\n",
      "260.4433644501379\n",
      "tensor([[-0.1257]])\n",
      "262.3866261592404\n",
      "tensor([[0.8411]])\n",
      "262.0797083683212\n",
      "tensor([[-0.1716]])\n",
      "266.547240707816\n",
      "tensor([[0.3204]])\n",
      "261.1198942501217\n",
      "tensor([[-0.2493]])\n",
      "266.71252061071647\n"
     ]
    }
   ],
   "source": [
    "envs=env\n",
    "\n",
    "early_stop = False\n",
    "\n",
    "k = 0\n",
    "cost, cum_reward = {}, {}\n",
    "\n",
    "from reward_function import reward_function\n",
    "\n",
    "episodes = 30\n",
    "for e in range(episodes):\n",
    "    state = envs.reset()\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "    done = False\n",
    "    i=0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        if i==0:\n",
    "            print(action)\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        reward=reward_function(reward)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        #print(masks[-1])\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        #frame_idx += 1\n",
    "        i=i+1\n",
    "        \n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    #print(len(returns))\n",
    "    #print(returns)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
    "    cost[e] = env.cost()\n",
    "    print(cost[e])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
