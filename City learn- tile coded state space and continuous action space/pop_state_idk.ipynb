{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn import  CityLearn, building_loader, auto_size\n",
    "from energy_models import HeatPump, EnergyStorage, Building\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "np.random.seed(3)\n",
    "\n",
    "import ray \n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only one building for SINGLE AGENT environment, unmark multiple building IDs to simulate MULTI-AGENT environment. In the multi-agent environment\n",
    "#the reward of each agent depend partially on the actions of the other agents or buildings (see reward_function.py)\n",
    "building_ids = [8]#, 5, 9, 16, 21, 26, 33, 36, 49, 59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building the RL environment with heating and cooling loads and weather files\n",
    "CityLearn\n",
    "    Weather file\n",
    "    Buildings\n",
    "        File with heating and cooling demands\n",
    "        CoolingDevices (HeatPump)\n",
    "        CoolingStorages (EnergyStorage)\n",
    "'''\n",
    "\n",
    "data_folder = Path(\"data/\")\n",
    "\n",
    "demand_file = data_folder / \"AustinResidential_TH.csv\"\n",
    "weather_file = data_folder / 'Austin_Airp_TX-hour.csv'\n",
    "\n",
    "heat_pump, heat_tank, cooling_tank = {}, {}, {}\n",
    "\n",
    "#Ref: Assessment of energy efficiency in electric storage water heaters (2008 Energy and Buildings)\n",
    "loss_factor = 0.19/24\n",
    "buildings = []\n",
    "for uid in building_ids:\n",
    "    heat_pump[uid] = HeatPump(nominal_power = 9e12, eta_tech = 0.22, t_target_heating = 45, t_target_cooling = 10)\n",
    "    heat_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    cooling_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    buildings.append(Building(uid, heating_storage = heat_tank[uid], cooling_storage = cooling_tank[uid], heating_device = heat_pump[uid], cooling_device = heat_pump[uid]))\n",
    "    buildings[-1].state_space(np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1, 40.0, 1.001]), np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 17.0, -0.001]))\n",
    "    buildings[-1].action_space(np.array([0.2]), np.array([-0.2]))\n",
    "    \n",
    "building_loader(demand_file, weather_file, buildings)  \n",
    "auto_size(buildings, t_target_heating = 45, t_target_cooling = 10)\n",
    "\n",
    "env = CityLearn(demand_file, weather_file, buildings = buildings, time_resolution = 1, simulation_period = (3500,6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_function import reward_function\n",
    "observations_space, actions_space = [],[]\n",
    "for building in buildings:\n",
    "    observations_space.append(building.observation_spaces)\n",
    "    actions_space.append(building.action_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(np.exp(x)/(1+np.exp(x)))\n",
    "\n",
    "\n",
    "def derivative_log_sigmoid(x):\n",
    "    der=(1/(sigmoid(x)-0.5))*sigmoid(x)*(1-sigmoid(x))\n",
    "    return(x)\n",
    "\n",
    "def montecarlo_return(rewards,gamma):\n",
    "    l=len(rewards)\n",
    "    r=0\n",
    "    for i in reversed(range(l)):\n",
    "        r=rewards[i]+(gamma**(l-i-1))*r\n",
    "    return(r)\n",
    "\n",
    "\n",
    "\n",
    "theta=np.random.rand(24)\n",
    "w=np.random.rand(24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reinforce montecarlo with baseline\n",
    "#problem:The agent is not able to pick up the action\n",
    "#action=tanh(xB)\n",
    "from reward_function import reward_function\n",
    "k = 0\n",
    "cost, cum_reward = {}, {}\n",
    "max_action=0.2\n",
    "episodes = 10\n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "\n",
    "for ep in range(10):\n",
    "    rewards_=[]\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    \n",
    "    state = env.reset()\n",
    "    state=state[0][:24]\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    t=0\n",
    "    #print(states)\n",
    "    j=0\n",
    "    while not done:\n",
    "        #if k%500==0:\n",
    "            #print('hour: '+str(k)+' of '+str(2500*episodes))\n",
    "        x=np.matmul(theta.T,state.T)\n",
    "        action = sigmoid(x)-0.5\n",
    "        next_state, reward, done, _ = env.step([[action]])\n",
    "        reward = reward_function(reward)[0] \n",
    "        \n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        rewards_.append(reward)\n",
    "        \n",
    "        state = next_state[0][:24]\n",
    "        \n",
    "        t=t+1\n",
    "        if done==True:\n",
    "            T=t\n",
    "        k+=1\n",
    "        \n",
    "    for t in range(T):\n",
    "        G=montecarlo_return(rewards_[t:],1)\n",
    "        s=states[t]\n",
    "        x=np.matmul(theta.reshape(1,24),s.reshape(24,1))[0][0]\n",
    "        v_s=np.matmul(w.reshape(1,24),s.reshape(24,1))[0][0]\n",
    "        delta=G-v_s\n",
    "        \n",
    "        w=w+(alpha*delta*s)\n",
    "        \n",
    "        der=derivative_log_sigmoid(x)\n",
    "        theta=theta+alpha*(gamma**t)*delta*der*s\n",
    "    cost[ep] = env.cost()\n",
    "    #print(theta)\n",
    "    print(cost[ep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one step actor critic\n",
    "def sigmoid(x):\n",
    "    return(np.exp(x)/(1+np.exp(x)))\n",
    "\n",
    "\n",
    "def derivative_log_sigmoid(x):\n",
    "    der=(1/(sigmoid(x)-0.5))*sigmoid(x)*(1-sigmoid(x))\n",
    "    return(x)\n",
    "\n",
    "def montecarlo_return(rewards,gamma):\n",
    "    l=len(rewards)\n",
    "    r=0\n",
    "    for i in reversed(range(l)):\n",
    "        r=rewards[i]+(gamma**(l-i-1))*r\n",
    "    return(r)\n",
    "\n",
    "\n",
    "theta=np.random.rand(24)\n",
    "w=np.random.rand(24)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.79261669771415\n",
      "186.79616611561832\n",
      "186.79877198160565\n",
      "186.80166149233835\n",
      "186.804851109327\n",
      "186.808358168026\n",
      "186.81220079035586\n",
      "186.8141681968384\n",
      "186.807645416465\n",
      "186.8240390508531\n"
     ]
    }
   ],
   "source": [
    "from reward_function import reward_function\n",
    "k = 0\n",
    "cost, cum_reward = {}, {}\n",
    "max_action=0.2\n",
    "episodes = 10\n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "\n",
    "\n",
    "for ep in range(10):\n",
    "    \n",
    "    rewards_=[]\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    \n",
    "    state = env.reset()\n",
    "    state=state[0][:24]\n",
    "    #states.append(state)\n",
    "    done = False\n",
    "    I=1\n",
    "    t=0\n",
    "    while not done:\n",
    "#         if k%500==0:\n",
    "#             print('hour: '+str(k)+' of '+str(2500*episodes))\n",
    "#         s=state\n",
    "        x=np.matmul(theta.T,state.T)\n",
    "        action = sigmoid(x)-0.5\n",
    "        #print(action)\n",
    "        next_state, reward, done, _ = env.step([[action]])\n",
    "        reward = reward_function(reward)[0] \n",
    "        #print(reward)\n",
    "        #actions.append(action)\n",
    "        #states.append(state)\n",
    "        #rewards_.append(reward)\n",
    "        \n",
    "        next_s=next_state[0][:24]\n",
    "        v_nextstate=np.matmul(w.reshape(1,24),next_s.reshape(24,1))[0][0]\n",
    "        v_state=np.matmul(w.reshape(1,24),s.reshape(24,1))[0][0]\n",
    "        \n",
    "        delta=reward+gamma*v_nextstate-v_state\n",
    "        #print('del',delta)\n",
    "        w=w+alpha*delta*s\n",
    "        \n",
    "        der=derivative_log_sigmoid(x)\n",
    "        #print('der',der)\n",
    "        theta=theta+alpha*I*delta*der*s\n",
    "        I=gamma*I\n",
    "        state = next_s\n",
    "        \n",
    "        t=t+1\n",
    "        if done==True:\n",
    "            T=t\n",
    "        k+=1\n",
    "   \n",
    "    cost[ep] = env.cost()\n",
    "    \n",
    "    print(cost[ep])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one step actor critic\n",
    "def sigmoid(x):\n",
    "    return(np.exp(x)/(1+np.exp(x)))\n",
    "\n",
    "\n",
    "def derivative_log_sigmoid(x):\n",
    "    der=(1/(sigmoid(x)-0.5))*sigmoid(x)*(1-sigmoid(x))\n",
    "    return(x)\n",
    "\n",
    "def montecarlo_return(rewards,gamma):\n",
    "    l=len(rewards)\n",
    "    r=0\n",
    "    for i in reversed(range(l)):\n",
    "        r=rewards[i]+(gamma**(l-i-1))*r\n",
    "    return(r)\n",
    "\n",
    "\n",
    "theta=np.random.rand(24)\n",
    "w=np.random.rand(24)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.74175124980292\n",
      "186.77020627103434\n",
      "186.75040819468515\n",
      "186.74051642953845\n",
      "186.73443955076004\n",
      "186.73067416594427\n",
      "186.72919454980928\n",
      "186.72972833531864\n",
      "186.73189883896129\n",
      "186.73502885309006\n",
      "186.73889669302557\n",
      "186.7426290927804\n",
      "186.74587221113936\n",
      "186.74929179582614\n",
      "186.7526568957964\n",
      "186.75654706522803\n",
      "186.7599488840644\n",
      "186.76316361590037\n",
      "186.76617885501895\n",
      "186.76830264948342\n",
      "186.7712153063862\n",
      "186.77463121943808\n",
      "186.77838792776794\n",
      "186.7804371838757\n",
      "186.78028074699245\n",
      "186.77966083534372\n",
      "186.7791396369079\n",
      "186.77898000106805\n",
      "186.7791389065797\n",
      "186.77806654834717\n",
      "186.7771873512363\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9b96637e0d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/harshita/Academics/semester 3/citylearnmajor/project/citylearn.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_resolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;31m#Heating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mbuilding_electric_demand\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbuilding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_storage_heating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0;31m#Cooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mbuilding_electric_demand\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbuilding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_storage_cooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/harshita/Academics/semester 3/citylearnmajor/project/energy_models.py\u001b[0m in \u001b[0;36mset_storage_heating\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \"\"\"\n\u001b[1;32m     44\u001b[0m         \u001b[0mheat_power_avail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheating_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_heating_power\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_source_heating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't_out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'heating_demand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mheating_energy_balance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheating_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'heating_demand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheat_power_avail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheating_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapacity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mheating_energy_balance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheating_energy_balance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'heating_demand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0melec_demand_heating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheating_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_electric_consumption_heating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheat_supply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheating_energy_balance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#actor critic with eligibility traces\n",
    "from reward_function import reward_function\n",
    "k = 0\n",
    "cost, cum_reward = {}, {}\n",
    "max_action=0.2\n",
    "episodes = 10\n",
    "alpha=0.5\n",
    "gamma=0.9\n",
    "\n",
    "\n",
    "for ep in range(1000):\n",
    "    rewards_=[]\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    \n",
    "    state = env.reset()\n",
    "    state=state[0][:24]\n",
    "    #states.append(state)\n",
    "    done = False\n",
    "    I=1\n",
    "    t=0\n",
    "    z_w=np.zeros(24)\n",
    "    z_theta=np.zeros(24)\n",
    "    R_bar=0\n",
    "    lambda_w=0.8\n",
    "    lambda_t=0.8\n",
    "    while not done:\n",
    "        #if k%500==0:\n",
    "            #print('hour: '+str(k)+' of '+str(2500*episodes))\n",
    "        s=state\n",
    "        x=np.matmul(theta.T,state.T)\n",
    "        action = sigmoid(x)-0.5\n",
    "        #print(action)\n",
    "        next_state, reward, done, _ = env.step([[action]])\n",
    "        reward = reward_function(reward)[0] \n",
    "        #print(reward)\n",
    "#         actions.append(action)\n",
    "#         states.append(state)\n",
    "#         rewards_.append(reward)\n",
    "        \n",
    "        next_s=next_state[0][:24]\n",
    "        v_nextstate=np.matmul(w.reshape(1,24),next_s.reshape(24,1))[0][0]\n",
    "        v_state=np.matmul(w.reshape(1,24),s.reshape(24,1))[0][0]\n",
    "        \n",
    "        delta=reward+gamma*v_nextstate-v_state-R_bar\n",
    "        der=derivative_log_sigmoid(x)\n",
    "        \n",
    "        R_bar=R_bar+alpha*delta\n",
    "        z_w=z_w*lambda_w*gamma+s\n",
    "        z_theta=z_theta*lambda_t*gamma+der*s*I\n",
    "        #print(delta)\n",
    "        #print(alpha*delta*z_w)\n",
    "        #print(alpha*I*delta*der*z_theta)\n",
    "        w=w+alpha*delta*z_w\n",
    "        theta=theta+alpha*I*delta*der*z_theta\n",
    "\n",
    "        state = next_s\n",
    "        \n",
    "        t=t+1\n",
    "        if done==True:\n",
    "            T=t\n",
    "        k+=1\n",
    "   \n",
    "    cost[ep] = env.cost()\n",
    "    #print(theta)\n",
    "    #if ep%100==0:\n",
    "    print(cost[ep])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
